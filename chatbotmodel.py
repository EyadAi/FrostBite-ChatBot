# -*- coding: utf-8 -*-
"""ChatBotModel.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1O-buhuaj1NknIlsJwWpQDRO3Xh_A6ODI
"""

import nltk
nltk.download('punkt_tab')
nltk.download('wordnet')

from google.colab import drive
drive.mount('/content/drive')

import numpy as np
import os
import random
import nltk
import tensorflow as tf
import keras as krs
import json
import pickle
from nltk.stem import WordNetLemmatizer
from nltk import word_tokenize
import pandas as pd
from IPython.display import display
import matplotlib.pyplot as plt


Lmzr = WordNetLemmatizer()
words = []
tags = []
documents = []
ignore_letters = ['?', '!', '.', ',']

file_path = 'intents.json'

with open(file_path, 'r', encoding='utf-8') as file:
    intents = json.load(file)

for tag in intents['intents']:
    for pattern in tag['patterns']:
        pattern_list = nltk.word_tokenize(pattern)
        words.extend(pattern_list)
        documents.append((pattern_list, tag['tag']))
        if tag['tag'] not in tags:
            tags.append(tag['tag'])

words = [Lmzr.lemmatize(word.lower()) for word in words if word not in ignore_letters]
words = sorted(set(words))
tags = sorted(set(tags))

training = []

for document in documents:
  max_len = len(max(document, key=len))
  print(max_len)
  for pattern_list in document:
    bag = []
    pattern_list = [Lmzr.lemmatize(word.lower()) for word in pattern_list]
    for word in words:
        bag.append(1) if word in pattern_list else bag.append(0)

    output_row = [0] * len(tags)
    output_row[tags.index(document[1])] = 1


    binarydocument = [bag, output_row]
    training.append(binarydocument)

random.shuffle(training)
training = np.array(training, dtype=object)

train_patterns = np.array([np.array(x, dtype=np.float32) for x in training[:, 0]])
train_tags = np.array([np.array(x, dtype=np.float32) for x in training[:, 1]])

model = krs.Sequential()
model.add(krs.layers.Dense(48, input_shape=(np.shape(train_patterns[0])), activation='relu'))
model.add(krs.layers.Dense(28, activation='relu'))
model.add(krs.layers.Dense(28, activation='relu'))
model.add(krs.layers.Dense(24, activation='relu'))
model.add(krs.layers.Dense(24, activation='relu'))
model.add(krs.layers.Dense(len(train_tags[0]), activation='softmax'))

model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
model.fit(train_patterns, train_tags, epochs=80)

Acc, Loss = model.evaluate(train_patterns, train_tags)

print("Accuracy ---> ", Acc)
print("Loss ---> ", Loss)

def sentense_cleaner(sentense):
    sentense_words = nltk.word_tokenize(sentense)
    sentense_words = [Lmzr.lemmatize(word.lower()) for word in sentense_words]
    return sentense_words

def bag_words(sentense, words):
    sentense_words = sentense_cleaner(sentense)
    bag = [0] * len(words)

    for sentense_word in sentense_words:
       for i, word in enumerate(words):
         if word == sentense_word:
          bag[i] = 1
    return np.array(bag)

def predict_tag(sentense):
    bow = bag_words(sentense, words)
    res = np.array([np.array(x, dtype=np.float32) for x in bow])
    pred = np.argmax(model.predict(np.array([res])))
    tag = tags[pred]
    for intent in intents['intents']:
        if intent['tag'] == tag:
            response = random.choice(intent['responses'])
    return response

!pip install gradio

import gradio as gr

iface = gr.Interface(
    inputs=gr.Textbox(lines=2, placeholder="Enter your message here..."),
    fn=predict_tag,
    outputs="text",
    title="FrostBite Chatbot",
    description="Interact with the chatbot and ask your questions.",
)

iface.launch()